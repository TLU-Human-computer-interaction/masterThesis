% !TeX root = thesis.tex
\documentclass{master_thesis}
\addbibresource{refs.bib}

\begin{document}

\section{Discussion} \label{chap:conclusions}
% The discussion chapter contextualizes the findings laid out in the previous chapter. What does the data mean for this topic? Did it fit into the theoretical framework? How does it change the way we think? These are the kinds of themes the discussion chapter expounds on.
% Feel free to talk about any surprises or unexpected results you had. Transparency is encouraged as a way to establish credibility, so this is a good place to share your personal opinions on how the research went.

The experience from the case study carried out as a part of this thesis, indicates that the tests fit well into the development workflow and don't cause any unnecessary complications - moreover, they help notice issues that are not only related to accessibility but may also indicate problems in code logic that could have otherwise been missed. This suggests that using automated accessibility testing this way has the potential to improve overall code quality in addition to ensuring that the testable accessibility issues get caught before changes are added to the codebase.

The evidence from relevant literature and this case study suggest that if possible, automated accessibility tests should be included in the development setup as early as possible. In the case of testing component libraries and using Storybook, an early start would definitely help avoid a lot of issues later on. For example, developers could write component examples to be more suitable for accessibility tests from the start to ensure that the tool can test for the maximum amount of issues possible.


% This seems like the best solution so instead of putting effort into building a custom tool the possibilities provided be the next version of the tool that is already being used should be used. Upgrading the component library to that version would need some extra effort, but the results from testing it in brick-ui and another new component library that was developed in Pipedrive during the case study would suggest that it would be worth it. Running the tests in \ac{ci} would ensure that they are run every time someone makes a change and not only when we choose to. We could also block changes that don't pass the required accessibility checks.


% \subsection{Limitations of the study}

% The tool that was used was chosen with the limitation of our current tooling. There are plenty of other options out there. Some free some paid and some of them might be more effective than the one we decided to use.

% We had limited time. The tool was in use for a bit more than 5 months. It will continue to be used and it would be interesting to see how well it performs long term.

% Manual accessibility audit might have not had enough highly qualified accessibility experts on the team.

% \subsection{Future research}

% Storybook 6 was used in Pipedrive's component library during the case study, but it seems that the next major version of this component explorer provides improved tools for testing. At the time of the study, some of these possibilities were tested to get an overview of what might be possible. The new major version has a feature to add play-functions or interactions to each story (example). These interactions are then run as unit tests. The assessability add-on works the same way and it is important to note that these accessibility tests will be run after the interaction finishes. This means that combining the add-on with interactions would allow testing of some states that were hidden before. For example, it would be possible to add user interaction to press a button for the 11 examples that had a button trigger and where the real component did not get tested. This scenario was tried out in the example component library and it works.

% Another added benefit that Storybook 7 provides is a test-runner is meant for running the interactions that can be added to each story as unit tests. Accessibility tests can be easily added to this globally without needing to do any extra steps in every component story. This will make it possible to run the same accessibility tests that can be seen in the add-ons panel in the \ac{cli} and the \ac{ci} testing phase.

% \todo{Adding accessibility tests from the beginning - would that have a bigger impact? }
% \todo{Possibilities in SB 7 could be explored, test runner would even allow to use of a different accessibility testing engine}
% \todo{AI for accessibility evaluation}

\subsection{Contributions}

Despite the limitations of this thesis, it has made a considerable contribution to the research of automated accessibility testing. Firstly, there had been no previous research done on using automated testing tools in component libraries. \ac{cdd} is a very common way of building web content and catching any issues related to accessibility in component libraries would ensure they are found as early in the process as possible.

Secondly, by doing a case study on a real UI building system, it brought out problems that others might face when they try to integrate automated testing tools into an already existing system. Building things in the correct way from the start is always easier than fixing them later.

\subsection{Limitations}

The study was carried out on an existing and in-use component library so all the changes to the library needed to be nonintrusive and not add any unnecessary complexity to the development workflow.

Another limitation that became apparent towards the end of the study was using Storybook 6 in the component library. The latest version would have provided much better possibilities, but it could not be used, because at the beginning of this research, this version was still in the first phases of development and a version this unstable could not have been used in a component library that is being actively developed. This was mitigated first by setting up a mock component library to try out any new options in a safe way and later by testing this new version in another development inside the company. Both of these examples provided valuable insights but were very limited on their own: the mock library did not use real examples and the second development was only observed over a short period.

The limitations of the manual accessibility audit on the component library were the time we had to go through all the components, the number of evaluators in the team and their level of expertise on the subject.

\subsection{Future Research}

At the time that the case study was being run, another new component library was built in Pipedrive and with the initial insights from the case study a better setup for evaluating accessibility could be implemented. Storybook 7 and its test-runner with accessibility tests were used. It will be interesting to see what the long-term effect of setting up these kinds of checks from the start of development will be. This means that there should be no need to rewrite components so that they would comply with accessibility standards, rather they would be built to comply from the start.

In this case study Storybook and axe-core were used as the main tools for running accessibility tests. It would be interesting to see how other tools perform in comparison to them. The test-runner in Storybook's latest version makes it possible to integrate any \ac{cli} tool to run tests on each component example. It would be even possible to run tests with multiple tools to potentially get better coverage of all automatically testable accessibility issues.

\subsection{Conclusion}
% As with all research paper conclusions, dissertation conclusions tie everything together. This chapter, the last of the core chapters, should reevaluate your thesis statement or clearly answer your research question. Remember not to present any new data or evidence in the conclusion, but rather review and reiterate the findings you presented earlier.

The goal of this thesis was to explore the possibilities of automated accessibility testing in component libraries. A case study that added an automated accessibility testing tool to Pipedrive's component library was carried out over 5 months.  Before the new tool was added, a survey was done to understand the context where this component library is being used. The results of the survey showed that accessibility is seen as something that is not currently the main focus of the company but should be prioritized more. The designers and developers in the company assess their knowledge on accessibility to be average or higher and many think that they need more resources on the subject.

To validate and compare the results from the automated testing, a manual audit was performed by a team of 3 designers and 1 developer. The results show that automated tests can be helpful, but the tools need to be carefully selected and set up to maximize the checks being run on each component. Not all accessibility issues can be caught by automated testing and testing a component library sets its own limitations by taking elements out of context.

There is no conclusive evidence from this research that automated accessibility testing in component libraries can reduce the number of accessibility issues on the website of other product that uses this library, but the results suggest that it might help bring more focus to the subject of accessibility and definitely catch some issues.

Storybook 6 and its add-on for accessibility testing were used in this research, but the possibilities of the next major version were also explored and this showed that


\end{document}