% !TeX root = thesis.tex
\documentclass{master_thesis}
\addbibresource{refs.bib}

\begin{document}

\section{Research} \label{chap:research}

The research conducted for this thesis can be divided into 6 steps:

\begin{enumerate}
	\item Researching automated testing tools
	\item Pre-case study questionnaire
	\item Setting up automated testing tools in Pipedrive's component library
	\item Manual accessibility audit
	\item Comparing manual and automated testing results
	\item Post-case study questionnaire
\end{enumerate}

The first thing was to look at different automated accessibility tools that are available, test them out and see what would best fit the purpose of this research. The tool needed to be easy to use for everyone and not block development. The next step was to send out a questionnaire to better understand the current approaches toward accessibility among the people who work on it the most. After that, an automated accessibility tool was added to the component library and announced in relevant channels to raise awareness about it.

In parallel to observing how the tool was being used a manual accessibility audit was conducted with a team of 3 designers and 1 developer on the same component library. These results were then compared with the automated testing results that were obtained from testing the same version of the library with an automated accessibility tool. As the last step, a questionnaire was sent out to gather information about how the tool had been adopted. To get more details some developers and designers that were known to have used the tool during that period were interviewed.

Pipedrive has been developing a sales \ac{crm} using mostly typescript and React. Accessibility has never been a high priority and at this point, it would not be very easy to get started. Pipedive has a design system and a React-based component library to keep the look and \ac{ux} consistent. This seemed like a good place to start solving accessibility issues. The library is used widely in the company and developers from different teams contribute to it. If a button in the reusable library gets fixed, most of the buttons in the web app that our customers use should be improved.

This should not be taken as a way to solve all accessibility problems but as a good first step to get started. Making changes in a reusable \ac{ui} library should have a wide impact on the product's overall accessibility and without establishing a basic level of accessibility there, it would be difficult to start testing individual pages of the final product.

\subsection{Research Methodology}

\info{Methodology – The overall strategy to conduct the research, e.g. experimental, quasi-experimental, correlational, descriptive, action research, design research, ethnography – Determined by the research question - summarise the research strategy and methodology for conducting the research.}

This chapter gives an overview of the research methodologies that were used to conduct this research and explains the reasoning behind choosing them and how they were employed.

% Methodologies:
%̶ S̶u̶r̶v̶e̶y̶ R̶e̶s̶e̶a̶r̶c̶h̶
%̶ E̶x̶p̶e̶r̶i̶m̶e̶n̶t̶a̶l̶ R̶e̶s̶e̶a̶r̶c̶h̶
%̶ D̶e̶s̶i̶g̶n̶ R̶e̶s̶e̶a̶r̶c̶h̶
% Action Research
% Case Studies

%  Methods
% – The tools and instruments that will be employed
% to gather evidence (e.g. collect data)
% • e.g. interview, questionnaire, observations, tests
% – Determined by the Methodology and Question

\subsubsection{Literature review}

A literature review is a critical analysis of existing research and scholarly literature on a particular topic. It involves systematically reviewing, evaluating, and synthesizing existing knowledge and research findings in a specific field to identify gaps in the current knowledge and identify topics for further research \citep{Luft2022}.

A literature review was conducted as a part of this research to understand the current landscape of automated accessibility testing. These kinds of tools have been available for a long time and the literature review part of this thesis intended to understand how much is known already know about their strengths and weaknesses. Have they been tested in real-world situations and what are the results? What is the difference between these tools? Have some been proven to be better than others?

The method for finding relevant scientific articles was to use EBESCO discovery service for Academic Library of Tallinn University to find any articles with keywords accessibility, automated evaluation and continuous integration. A similar search was conducted on Google to find any nonscientific articles about the subject. The references in all of these articles were explored to identify any relevant research that should also be included.

This produced sufficiently extensive results and very soon the conclusions and references in the articles started to repeat themselves. The tools that are used evolve fast and this would make older articles irrelevant to the current state of continuous accessibility testing. As a general rule, articles that were published in the last 10 years were focused on and older ones were only looked at when they gave more high-level overviews or methods and did not focus on comparing specific tools.

Any research papers on the subject of automated accessibility testing and any relevant articles or blog posts were looked through. Things change fast in software development and it could not be expected to find the most up-to-date information from scientific publications. That's why it was thought to be necessary to also include resources that have been published on reputable sites.

Many studies \citep{Alsaeedi2020, Ismailova2022, Sane2021, Vigo2013, RybinKoob2022, Duran2017} have been done to compare different accessibility testing tools using various methods. The results of these studies were looked at and enough tools were assessed as a part of the exploratory phase to find one that would be usable for this case study, but the intention was not to compare them methodically as a part of this work. The comparisons that have been made by others in the past will be used as a foundation to decide what tools to use.

\subsubsection{Case study}

A case study is an in-depth analysis of a bounded system \citep{Range2023}. It involves multiple forms of data collection like observations, interviews, documents, reports and analysis. The case can be a specific individual, group, community, business, organization, event or phenomenon. It can be chosen because of its uniqueness or typicality. The goal of using case study methodology is to investigate something contemporary in its real-life context.

The results of a single case study might be very subjective to that particular case and to the biased opinions of the researcher and can't always be generalized and applied to other similar situations, but the richness of detail they provide makes them fascinating and often there is a lot to learn from them \citep{Range2023}. Sometimes these insights can be applied to other similar cases. It is a good method for exploratory or critical and unusual cases.

This method was chosen to explore the possibilities of automated accessibility evaluation. An organizational case study was conducted that focused on the process of evaluating and improving the accessibility of \ac{crm} tool called Pipedrive. In the scope of this study, an automated accessibility evaluation tool was added to the component library of Pipedrive and a manual accessibility evaluation of the same library was conducted with a team of 3 designers and 1 developer working in the company.

Before implementing the new tool a questionnaire was sent out to understand the knowledge about and approaches towards the subject of web accessibility in the company among people who are most likely to be doing work related to it. At the end of the case study another questionnaire was sent out with a focus on gaining information from people in the organization who use the tool that was added during that period. The aim of this was to understand if the tool was helpful and if they had any issues using it and what other opinions they might have about it. The goal of both of these was to get more detailed information about the experience of the real users of the tool.

The data collected from automated and manual testing was organized to make it comparable. Statistical analysis was used to gain valuable insight into how well these two methods work and how they might differ from one another.

\subsection{Current state of awareness about accessibility in Pipedrive}

\info{Answer to RQ1: How good is the knowledge about accessibility standards, tools and best practices in the company before integrating the accessibility testing tool?}

As the first step of the research, a survey was sent out in Pipedive's Slack channels (Figure \ref{fig:slack-message}) to understand what is the knowledge and general approach to web accessibility in the company. It was shared in 4 channels to reach people who are most likely to be dealing with accessibility, the component library and who are interested in accessibility (Table \ref{table:survey-shared}).

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/survey.png}
	\caption{Message in company Slack announcing adding accessibility tool and inviting people to reply to the survey.}
	\label{fig:slack-message}
\end{figure}

\begin{table}[H]
	\centering
	\caption{List of all the channels the survey was shared in.}

	\begin{tabular}{|l|l|}
		\hline
		\textbf{Channel members or theme} & \textbf{number of members}  \\
		\hline
		Front end developers  & 212  \\
		\hline
		Dedicated to our component library  & 124  \\
		\hline
		Designers  & 73  \\
		\hline
		Accessibility channel  & 31  \\
		\hline
	\end{tabular}
	\label{table:survey-shared}
\end{table}

Some people might also be on more than one of these channels. The aim was to reach people in the company who would be most likely to be using these tools and contribute to the library. There were 7 questions and some of them also included a field for free text to give more details on the subject if they wanted (Appendix \ref{appendix:pre-survey-questions}).

In total, 20 people replied to the survey - 6 designers and 14 developers, including one engineering manager. This does not give a full overview of the company, but it should give a good insight into the general opinions regarding accessibility. Likely, developers and designers that are more involved with our component library and/or are interested in accessibility were more likely to respond.

The results show that 10\% of people who responded think their knowledge of accessibility is very good, while most think that their knowledge level is average and none of the responders judge their knowledge about accessibility to be very good (Figure \ref{fig:a11y-knowledge-current}). 35\% of people who participated in the survey know where to find resources about accessibility standards, 15\% don't know and 50\% know, but think they need more (Figure \ref{fig:a11y-resources}).

\begin{figure}[H]
    \centering
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\textwidth]{img/a11y-knowledge.png}
		\caption{Pre-case study survey: How much do you know about accessibility (standards, best practices, importance, testing)? 1- "Almost no knowledge" to 5 - "I have very good knowledge" }
		\label{fig:a11y-knowledge-current}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth]{img/a11y-resources.png}
		\caption{Do you know where to find resources
		about accessibility standards? (no one chose the 4th option "No I don't think I need them") }
    	\label{fig:a11y-resources}
	\end{subfigure}
	\caption{Accessibility knowledge in Pipedrive}
    \label{fig:a11y-knowledge}
\end{figure}

Most people who replied to the survey don't think accessibility is being prioritized in Pipedrive and at the same time, most of them think that Pipedrive should be putting more focus on following common accessibility standards (Figure \ref{fig:a11y-priority}). They elaborate a bit more about what they think are the reasons behind this in the free text answers to questions 4 and 6.

According to these replies, accessibility has always been at the end of Pipedrive's priorities and there is no clear company-wide strategy on how to manage and improve it. There has never been a dedicated project manager to bring focus to this subject
and right now developers and designers are the ones that seem to be responsible for it. Two people also mentioned that the priority might be low because as a private service, Pipedrive does not have a legal obligation to comply with any specific accessibility standard. Two people think that it would not affect Pipedrive's customers much. All replies can be seen in  Appendix \ref{appendix:post-survey-responses}.

\begin{figure}[H]
    \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/a11y-priority.png}
		\caption{Do you think accessibility is a priority in our company? 1-No not at all to 5-Yes very much \\
		\\
		\\}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/a11y-goals.png}
		\caption{Do you think we should prioritize following accessibility standards (like \ac{wcag}, EN 301 549) in our product? 1- "No I think they are irrelevant for Pipedrive customers" to 5 - "Yes I think following accessibility standards should be a high priority" }
	\end{subfigure}
	\caption{Priority of accessibility in Pipedrive}
    \label{fig:a11y-priority}
\end{figure}

It seems like the biggest obstacle to improving the accessibility of Pipedrive is the lack of knowledge about the business impact. More knowledge would help prioritize solving these problems. There is a lot currently that could be improved, but the evidence would suggest that implementing a way of continuously testing for at least some basic issues from the beginning of all new developments would help to ensure that the technical debt related to accessibility would not grow to be even bigger. There were several comments in the free text sections expressing saying they appreciate this subject being brought to focus and even some offering to help.

\subsection{Adding Automated Testing to Pipedrive's Component Library} \label{adding-a11y-tool}

The next step was adding some automated tests to Pipedrive's component library development workflow and observing their usefulness. The aim was to find something that can be integrated easily and that does not have a steep learning curve or add unnecessary complexity to the development workflow. Two open-source \ac{cli} tools that were mentioned in the previous research were tested to see if they would be suitable.

Pa11y CI is an accessibility test runner that is focused on \ac{ci} environments \citep{TeamPa11y2022}. It is said to work very well in \ac{ci}. The tool supports \ac{wcag} 2.0 A, AA and AAA standards and the tests can be run on multiple URLs. Axe-core is an accessibility testing engine for websites and other HTML-based user interfaces \citep{Deque2023}. It supports \ac{wcag} 2.0 and 2.1 on levels A, AA, AAA, but needs a bit more initial setup to start testing webpages.

Storybook is used as the component explorer for our reusable component library's development. It is open-source software for \ac{ui} development that allows teams to work on one component at a time \citep{storybook}. It allows us to render isolated \ac{ui} components without integrating them into the final product right away. The developers in Pipedrive use it to preview the components they are developing locally. A version of Storybook is also published internally with all the components available for anyone in the company to see.

To run these tests with either tool the components first need to be rendered in a browser. This can be done by using Storybook. Each component has stories – examples of how the component would be used in real life. These stories will be rendered in a browser inside Storybook \ac{ui}. It also has a sidebar for navigating between different examples, and controls for additional tools and documentation.
This means that to test the isolated components with pa11y CI or axe-core Storybook's \ac{ui} that gets rendered around the component needs to be excluded so we can run the tests on only the actual relevant example.

Both of these tools are more suitable for testing whole web pages. Testing isolated components with the current setup would have required some extra steps. First, a list of all the examples for each component together with their URLs would need to be obtained in a way that it can be kept up to date every time changes are made. The next challenge would have been to isolate the components in Storybook UI to test only them and not the UI around them. This all seemed like an extensive thing to build and there probably were better more suitable solutions available already, because component libraries and using Storybook for developing them is quite common.

Storybook has a wide ecosystem of add-ons and one of them is addon-a11y. It uses the same accessibility testing engine, axe-core to audit the \ac{html} rendered for an isolated component \citep{addon-a11y}. The issues that were mentioned before have already been solved here. The add-on tests only the relevant part of the \ac{html} and also provides a nice \ac{ui} inside Storybook (Figure \ref{fig:addon-a11y}).

\begin{figure}[h]
	\includegraphics[width=\textwidth]{img/addon-a11y.png}
	\caption{Accessibility add-on (addon-a11y) in Storybook. The accessibility panel is highlighted in yellow and the component example (story) is highlighted in green.}
	\label{fig:addon-a11y}
\end{figure}

As mentioned before axe-core is an accessibility testing engine for websites and other \ac{html}-based user interfaces that supports \ac{wcag} 2.0 and 2.1 on levels A and AA \citep{Deque2023}. It has a coverage of about 20-30\% if you calculate it in a commonly used way, but they claim it to be 57\% according to their new and improved calculation method. This has been explained in more detail in \nameref{continuous-a11y-evaluation} chapter.

This seemed like the best solution in our situation so addon-a11y was added to our component library’s Storybook. The accessibility tab is visible in every component example. It shows all the checks that the component has passed, all the violations that were found and any issues that could not be checked and might need manual testing. Every rule listed there also has reference to the \ac{html} node that had the violation, explanation and links to the Deque webpage with examples. This should be very useful in understanding and fixing these issues.

The rules format that axe-core uses is developed by Deque Systems and is an adoption of the \ac{act} rules format developed by \ac{wai} \citep{Fiers2017}. They have a set \ac{wcag} Success Criteria that can be evaluated in a fully automated way. They are divided by \ac{wcag} standards version (2.0, 2.1, 2.2) and Level (A \& AA, AAA) and they also have some rules for best practices in the industry that improve the user experience but might not conform to \ac{wcag} success criterion \citep{Fiers2023}.

The addon in Storybook provided a good way to test all the examples, but no way to get an overview of how many issues we have in the whole library. A summary of all the potential accessibility problems was needed to evaluate the effort that would be needed to make the components provided by this library accessible and to measure the impact of making improvements there.

To get an overview of all the issues Storybook-a11y-report \citep{Karube2020} was used to generate a report of all the violations in the whole component library. This \ac{cli} tool is meant to be used together with addon-a11y and it goes through all the examples and runs the same tests as addon-a11y to generate a summarized report. The final report links back to each component's story where you can see the example and addon panel with all the information mentioned before (Appendix \ref{appendix:report}).

To get some data that could be compared with the results from manual testing, all the examples were examined in detail and the following data were extracted for each component (Appendix \ref{appendix:results-table}):
\begin{itemize}
	\item \textbf{unique violations detected for component} - A list of unique violations visible in components Accessibility tab
	\item \textbf{report\_violations} - Count of how many times the component was mentioned in the report that included all the components with all their examples. There might have been duplicates here when the same issue is visible in more than one example for the same component.
	\item \textbf{violations} - Count of all unique violations detected for the component. One violation reported in many examples was only counted once.
	\item \textbf{true\_violations} - Count of how many of these violations are valid and relevant to this component. All the violations in each component example were looked at to decide if they were relevant and valid. All violations that come for other components or were related to elements added just to illustrate the usage of the component were excluded.
	\item \textbf{passes detected for component} - A list of unique passes visible in components Accessibility tab
	\item \textbf{passes} - Count of all unique passes detected for the component. One pass reported in many examples was only counted once.
	\item \textbf{true\_passes} - Count of how many of these passes are valid and relevant to this component. All the passes in each component example were looked at to decide if they were relevant and valid. All passes that come for other components or were related to elements added just to illustrate the usage of the component were excluded.
	\item \textbf{explanation of false violations and passes} - An explanation of why each false violation and pass was not considered relevant
	\item \textbf{violations from manual testing for component} - A list of violations listed for each component in the manual audit report. Left out issues that were detected using addon-a11y, because these are listed in automated testing.
	\item \textbf{manual\_count} - Count of these manually found violations
\end{itemize}

At the same time, a very simple component library, brick-ui was set up to test anything that might not be very easy to try out in a real component library that is widely used and includes more than 50 components. This library was used as a quick playground to test out ideas and it provided an easy way to see if the latest version of Storybook which was still in beta at the beginning of the case study would have solved some of the issues that were found. Continuous improvements are being made in both  Storybook and axe-core through the maintainers and active communities around them. Brick-ui provided an opportunity to set up simple examples to illustrate some questions that came up while setting up the tooling. Examples from Pipedrie's component library could not be shared, because the code is not publicly available. As a result, Storybook's maintainers advised to use pseudo states addon for testing different states of components and a bug that was found during testing could be reported to axe-core maintainers.

While this research was going on another new component library development started in Pipedrive and because the latest versions of Storybook had been explored in the simple component library the team had the confidence to use this prerelease version of the library as the component explorer of this new library. This provided some invaluable insight into an improved automated accessibility testing workflow inside Storybook and also an opportunity to observe what benefits could setting up automated accessibility tests from the start of development have on the final result. In this scenario, there would be no need to fix already existing issues, but rather the developers would be able to avoid them from the start.

\subsection{Manual Accessibility Audit}

The next part was conducting a manual accessibility audit of the same library to get a better overview of all the issues. This approach will enable a comparative analysis of the results of the manual testing and the automatically generated test report to determine what are the tool's strengths and weaknesses and if testing isolated components poses any limitations.

Storybook was used to render an isolated preview of each component. The accessibility add-on had already been installed, and all the violations reported there were also explored. The team that conducted the audit consisted of 4 people - 3 designers and 1 developer.
There was one designer in the team who considered himself to be an accessibility expert and the rest had a lower level of knowledge. \citeauthor{Brajnik2011} warned about a significant drop in validity when testing with non-experts compared to experts \citep{Brajnik2011}. According to him, at least 3 expert evaluators should be used to get reliable results and when the evaluators are not experts then there should be at least 14 to reach the same quality in results. The audit conducted as a part of this research had 4 evaluators and not all of them can be considered experts, but they all had a basic level of knowledge in the field of accessibility. Daily discussions and 2 longer review sessions were carried out to mitigate the lack of experience and ensure that the results from the audit are as good as possible.

Tasks were created for each component - 53 tasks in total. Everyone tested the components individually and wrote down any issues they found in a shared table. Each morning the team had a meeting to discuss any questionable issues and to share interesting insights. Each evaluator checked violations in the accessibility add-on panel, using a keyboard, and also a screen reader to navigate. Each component had 1 or more examples and the team members looked over as many of them as they thought was necessary to get an extensive overview of that specific component.

At the beginning of the audit, an add-on in Storybook for mocking a screen reader \citep{Lara} was used. It showed the output that a screen reader would play as audio, in text form in the add-ons panel in Storybook. Initially, it seemed like a convenient solution with rather reliable results, but further investigation revealed that the output was very different from what an actual screen reader would say.

During the rest of the audit, VoiceOver - the built-in screen reader for Apple products was used because our work computers are MacBooks and VoiceOver was conveniently available for us to use. There was an initial learning curve, but after that, it went quite smoothly. All the components that had already been tested with the faulty add-on were reviewed again using VoiceOver.

The audit results were documented in a table. The barrier walkthrough method where the barriers that were focused on were defined by how the user interacts with the webpage was used. Evaluators looked at what issues different types of users might encounter with each component and separated the results into 3 sections:
\begin{enumerate}
	\item Mouse user issues - using a mouse to interact with the page
	\item Keyboard user issues - using only a keyboard to do the same things
	\item Screen reader user issues - trying to do the same things and getting the same information while only using a screen reader
\end{enumerate}

This separation was seen as a good way to prioritize fixing the issues in the future. The mouse user is the user who is currently considered in all of our development. The issues they would encounter should be the most critical. This category includes a lot of visual, color contrast, and click target size issues.

The second type of user would encounter all the issues from the first category plus everything that is unusable for them by using a keyboard. The functionalities that the user should be able to use with a mouse were explored by using only the keyboard.

The third user was imitated using a screen reader. The prerequisite for this was keyboard accessibility - if it was not usable with a keyboard then it might not be usable by a screen reader. Some components actually worked better with a screen reader. Screen readers have more possibilities for navigating through elements and nested elements and this helps access come nested focusable elements that were unreachable by using regular keyboard navigation.

In real life, these 3 types of users might not be so clearly separated and many issues would affect all types of users, but as the intention was to come out of the audit with an actionable list, the issues needed to be prioritized in order of severity and current customer impact. These categories also mostly depend on each other, so it would make sense in most cases to start by solving mouse user issues, then keyboard user issues and then screen reader user issues.

\end{document}